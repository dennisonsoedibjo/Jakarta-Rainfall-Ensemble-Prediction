# -*- coding: utf-8 -*-
"""Improved Jakarta Rainfall Prediction Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GF4x36XaXrZcVnW0Iyq8fwUOAhEnd7fl
"""

pip install bayesian-optimization

"""# **1. Data Loading and Initial Preprocessing**"""

import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('Jakarta_Rainfall_Dataset.csv', parse_dates=['date'], dayfirst = True)
df.set_index('date', inplace=True)

print("Initial DataFrame Info:")
df.info()

# Check the percentage of missing values in each column
print("Missing values percentage before dropping NaNs:")
print(df.isnull().mean() * 100)

# Handle missing categorical data (most_wind_direction)
if df['most_wind_direction'].isnull().any():
    print("Imputing missing 'most_wind_direction' with mode...")
    mode_wind_direction = df['most_wind_direction'].mode()[0]
    df['most_wind_direction'].fillna(mode_wind_direction, inplace=True)
    print(f"'most_wind_direction' NaNs after mode imputation: {df['most_wind_direction'].isnull().sum()}")

# Drop rows where 'rainfall' (target variable) is NaN
# It's crucial not to impute the target variable for training
initial_rows = df.shape[0]
df.dropna(subset=['rainfall'], inplace=True)
print(f"Removed {initial_rows - df.shape[0]} rows due to missing 'rainfall' values.")

"""### Apply KNNImputer for numerical columns


"""

from sklearn.impute import KNNImputer

numerical_cols = df.select_dtypes(include=np.number).columns.tolist()

# Exclude 'rainfall' from imputation as it's already dropped for NaNs
if 'rainfall' in numerical_cols:
    numerical_cols.remove('rainfall')

# Print only columns that still have NaNs in the selected numerical columns
print("Missing numerical values before KNNImputation:")
missing_before_imputation = df[numerical_cols].isnull().sum()
print(missing_before_imputation[missing_before_imputation > 0])


if numerical_cols: # Only proceed if there are numerical columns to impute
    imputer = KNNImputer(n_neighbors=5)
    # Perform imputation on a temporary DataFrame containing only the numerical columns for efficiency
    temp_df_numerical = df[numerical_cols]
    df[numerical_cols] = imputer.fit_transform(temp_df_numerical)

print("\nMissing numerical values after KNNImputation:")
missing_after_imputation = df[numerical_cols].isnull().sum()
print(missing_after_imputation[missing_after_imputation > 0]) # Should be all zeros now

print(f"\nDataFrame shape after imputation: {df.shape}")

"""# **2. Feature Engineering**"""

# Create interaction features
df['temp_humidity_interaction'] = df['avg_temperature'] * df['avg_humidity']
df['rainfall_per_sunshine'] = df['rainfall'] / df['sunshine_duration']

# Add rolling statistics for 'rainfall' and 'avg_temperature'
for window in [3, 7, 14]:
    df[f'rainfall_rolling_mean_{window}d'] = df['rainfall'].rolling(window=window).mean()
    df[f'rainfall_rolling_std_{window}d'] = df['rainfall'].rolling(window=window).std()
    df[f'temp_rolling_mean_{window}d'] = df['avg_temperature'].rolling(window=window).mean()
    df[f'temp_rolling_std_{window}d'] = df['avg_temperature'].rolling(window=window).std()

# Add seasonal decomposition features
from statsmodels.tsa.seasonal import seasonal_decompose

try:
    temp_series = df['rainfall'].dropna() # Ensure no NaNs before seasonal decomposition that would cause it to fail
    if not temp_series.empty:
        print("Creating seasonal decomposition features...")
        seasonal_decomp = seasonal_decompose(temp_series, period=365, extrapolate_trend='freq')
        # Reindex the decomposed components to match the original DataFrame's index
        df['trend'] = seasonal_decomp.trend.reindex(df.index)
        df['seasonal'] = seasonal_decomp.seasonal.reindex(df.index)
        df['residual'] = seasonal_decomp.resid.reindex(df.index)
        print("Seasonal decomposition features created successfully.")
    else:
        print("Rainfall series is empty after dropping NaNs for seasonal decomposition. Skipping.")
        df['trend'] = np.nan
        df['seasonal'] = np.nan
        df['residual'] = np.nan
except Exception as e:
    print(f"Seasonal decomposition failed: {e}. Skipping this feature.")
    df['trend'] = np.nan
    df['seasonal'] = np.nan
    df['residual'] = np.nan

# Drop NaNs introduced by new features
# This dropna is fine as it's after imputation and targets are already clean.
df.dropna(inplace=True)
print(f"DataFrame shape after feature engineering NaNs: {df.shape}")

"""# **3. Outlier Removal (using IQR with capping/winsorization method)**"""

outlier_log = {}
numeric_cols = df.select_dtypes(include='number').columns

for col in numeric_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Store the number of values that will be capped
    before_capping_lower = (df[col] < lower_bound).sum()
    before_capping_upper = (df[col] > upper_bound).sum()

    # Apply capping/winsorization
    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)

    # Log the number of values capped for each column
    outlier_log[col] = before_capping_lower + before_capping_upper

print("Outlier capping summary (values capped by column):")
for k, v in outlier_log.items():
    if v > 0:
        print(f"   {k}: {v} values capped")
print(f"\nDataFrame shape after outlier capping: {df.shape}")

# outlier_log = {}
# numeric_cols = df.select_dtypes(include='number').columns
# for col in numeric_cols:
#     Q1 = df[col].quantile(0.25)
#     Q3 = df[col].quantile(0.75)
#     IQR = Q3 - Q1
#     lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR
#     before = len(df)
#     df = df[(df[col] >= lower) & (df[col] <= upper)]
#     after = len(df)
#     outlier_log[col] = before - after

# print("Outlier removal summary (rows removed by column):")
# for k, v in outlier_log.items():
#     if v > 0:
#         print(f"  {k}: {v} rows")
# print(f"\nDataFrame shape after outlier removal: {df.shape}")

"""# **4. Time-based and Categorical Features**"""

# Ensure the index is a DatetimeIndex before creating cyclical features
if not isinstance(df.index, pd.DatetimeIndex):
    print("Converting index to DatetimeIndex...")
    df.index = pd.to_datetime(df.index)
    print("Index converted successfully.")

# Create cyclical time features
df['day_sin'] = np.sin(2 * np.pi * df.index.dayofyear / 365.0)
df['day_cos'] = np.cos(2 * np.pi * df.index.dayofyear / 365.0)
df['month_sin'] = np.sin(2 * np.pi * df.index.month / 12.0)
df['month_cos'] = np.cos(2 * np.pi * df.index.month / 12.0)
df['weekday_sin'] = np.sin(2 * np.pi * df.index.weekday / 7.0)
df['weekday_cos'] = np.cos(2 * np.pi * df.index.weekday / 7.0)


# Create lag features for 'rainfall' (target variable)
for lag in [1, 2, 3, 6, 12, 24, 365]:
    df[f'rainfall_lag_{lag}'] = df['rainfall'].shift(lag)

# Add lag features for 'avg_temperature' and 'avg_humidity'
for lag in [1, 3, 7]:
    df[f'avg_temperature_lag_{lag}'] = df['avg_temperature'].shift(lag)
    df[f'avg_humidity_lag_{lag}'] = df['avg_humidity'].shift(lag)

df.dropna(inplace=True) # Drop NaNs introduced by lag features
print(f"DataFrame shape after lag feature engineering: {df.shape}")

# Apply one-hot encoding to 'most_wind_direction'
df = pd.get_dummies(df, columns=['most_wind_direction'], prefix='wind_dir', dummy_na=False)
print("DataFrame Info after one-hot encoding:")
df.info()

"""# **5. Feature Selection (Mutual Information)**"""

from sklearn.feature_selection import mutual_info_regression

X = df.drop(columns=['rainfall'])
y = df['rainfall']

# Convert boolean columns to integer for mutual_info_regression
for col in X.select_dtypes(include='bool').columns:
    X[col] = X[col].astype(int)

mi = mutual_info_regression(X, y, random_state=42)
mi_scores = pd.Series(mi, index=X.columns).sort_values(ascending=False)

# Select top 20 features
selected_features = mi_scores.head(20).index.tolist()

X = df[selected_features]
print(f"Selected {len(selected_features)} features based on Mutual Information:")
X.info()

"""# **6. Data Scaling**"""

from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)
# X_scaled is a numpy array now, not a DataFrame.

"""# **7. Plotting Selected Features**"""

import matplotlib.pyplot as plt

# Plot selected features over time
numeric_selected_features = X.select_dtypes(include='number').columns # Exclude boolean values because it might distort plots
X[numeric_selected_features].plot(subplots=True, layout=(5, 4), figsize=(40, 20), sharex=False)
plt.suptitle('Selected Numeric Features Over Time', y=1.02, fontsize=30)
plt.tight_layout(rect=[0, 0, 1, 0.98])
plt.show()

"""# **8. Model Architecture and Training with TimeSeriesSplit CV**"""

from sklearn.model_selection import TimeSeriesSplit, cross_val_score

# TimeSeries CV setup
tscv = TimeSeriesSplit(n_splits=5)
rf_preds, xgb_preds, lstm_preds, y_valids = [], [], [], []
meta_features = [] # To store out-of-fold predictions for the meta-learner

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
import tensorflow as tf

# LSTM architecture
def create_lstm_model(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        LSTM(128, return_sequences=True, activation='relu'),
        Dropout(0.3),
        LSTM(64, activation='relu'),
        Dropout(0.2),
        Dense(32, activation='relu'),
        Dense(1)
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='mse',
        metrics=['mae']
    )
    return model

from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Early stopping and learning rate scheduler callbacks
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=0.0001
)

from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

# Define objective functions for Bayesian Optimization
# Random Forest objective
def rf_objective(n_estimators, max_depth, min_samples_split, min_samples_leaf):
    rf_model = RandomForestRegressor(
        n_estimators=int(n_estimators),
        max_depth=int(max_depth),
        min_samples_split=int(min_samples_split),
        min_samples_leaf=int(min_samples_leaf),
        random_state=42,
        n_jobs=-1
    )
    # Use negative MSE as Bayesian Optimization maximizes
    return -np.mean(cross_val_score(rf_model, X_scaled, y, cv=tscv, scoring='neg_mean_squared_error', n_jobs=-1))

# XGBoost objective
def xgb_objective(n_estimators, max_depth, learning_rate, subsample, colsample_bytree, gamma):
    xgb_model = XGBRegressor(
        n_estimators=int(n_estimators),
        max_depth=int(max_depth),
        learning_rate=learning_rate,
        subsample=subsample,
        colsample_bytree=colsample_bytree,
        gamma=gamma,
        random_state=42,
        n_jobs=-1
    )
    return -np.mean(cross_val_score(xgb_model, X_scaled, y, cv=tscv, scoring='neg_mean_squared_error', n_jobs=-1))

"""### Bayesian Optimization for Random Forest"""

from bayes_opt import BayesianOptimization

print("--- Bayesian Optimization for Random Forest ---")
rf_pbounds = {
    'n_estimators': (100, 500),
    'max_depth': (5, 25),
    'min_samples_split': (2, 10),
    'min_samples_leaf': (1, 5)
}
rf_optimizer = BayesianOptimization(f=rf_objective, pbounds=rf_pbounds, random_state=42)
rf_optimizer.maximize(init_points=10, n_iter=25) # Increased iterations for better tuning
best_rf_params = rf_optimizer.max['params']
print(f"Best RF params: {best_rf_params}")

"""### Bayesian Optimization for XGBoost"""

print("--- Bayesian Optimization for XGBoost ---")
xgb_pbounds = {
    'n_estimators': (100, 500),
    'max_depth': (3, 15),
    'learning_rate': (0.01, 0.2),
    'subsample': (0.5, 1.0),
    'colsample_bytree': (0.5, 1.0),
    'gamma': (0, 0.5)
}
xgb_optimizer = BayesianOptimization(f=xgb_objective, pbounds=xgb_pbounds, random_state=42)
xgb_optimizer.maximize(init_points=10, n_iter=25) # Increased iterations
best_xgb_params = xgb_optimizer.max['params']
print(f"Best XGBoost params: {best_xgb_params}")

# Convert best params to int where needed
best_rf_params['n_estimators'] = int(best_rf_params['n_estimators'])
best_rf_params['max_depth'] = int(best_rf_params['max_depth'])
best_rf_params['min_samples_split'] = int(best_rf_params['min_samples_split'])
best_rf_params['min_samples_leaf'] = int(best_rf_params['min_samples_leaf'])

best_xgb_params['n_estimators'] = int(best_xgb_params['n_estimators'])
best_xgb_params['max_depth'] = int(best_xgb_params['max_depth'])

print("--- Training Base Models with TimeSeriesSplit and Cross-validated Stacking ---")
# This loop performs cross-validated stacking
for train_idx, val_idx in tscv.split(X_scaled):
    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx] # Use .iloc for pandas Series

    # Random Forest
    rf = RandomForestRegressor(**best_rf_params, random_state=42)
    rf.fit(X_train, y_train)
    rf_val_preds = rf.predict(X_val)
    rf_preds.append(rf_val_preds)

    # XGBoost
    xgb = XGBRegressor(**best_xgb_params, random_state=42)
    xgb.fit(X_train, y_train)
    xgb_val_preds = xgb.predict(X_val)
    xgb_preds.append(xgb_val_preds)

    # LSTM
    # Reshape for LSTM: [samples, time_steps, features]
    # Here, time_steps is 1 as we are predicting current rainfall from current features + lags
    X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
    X_val_lstm = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))

    lstm = create_lstm_model(input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]))
    lstm.fit(X_train_lstm, y_train,
             epochs=50, batch_size=32, verbose=0,
             validation_data=(X_val_lstm, y_val),
             callbacks=[early_stopping, lr_scheduler]) # Added callbacks
    lstm_val_preds = lstm.predict(X_val_lstm).flatten()
    lstm_preds.append(lstm_val_preds)

    # Store out-of-fold predictions for meta-learner training
    # Stack predictions horizontally for the meta-learner
    fold_meta_features = np.column_stack((rf_val_preds, xgb_val_preds, lstm_val_preds))
    meta_features.append(fold_meta_features)

    y_valids.append(y_val) # Store corresponding true values

# Concatenate all out-of-fold predictions and true values
stacked_X = np.vstack(meta_features)
y_true_stacked = np.concatenate(y_valids)

"""# **9. Stacking Layer (Meta-model)**"""

from sklearn.linear_model import LinearRegression

print("--- Training Meta-Model (Linear Regression) ---")
meta_model = LinearRegression()
meta_model.fit(stacked_X, y_true_stacked)

"""# **10. Final Predictions on the Entire Dataset**"""

# Re-train base models on full X_scaled for final predictions
final_rf = RandomForestRegressor(**best_rf_params, random_state=42)
final_rf.fit(X_scaled, y)

final_xgb = XGBRegressor(**best_xgb_params, random_state=42)
final_xgb.fit(X_scaled, y)

# Reshape full data for LSTM
X_scaled_lstm = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))
final_lstm = create_lstm_model(input_shape=(X_scaled_lstm.shape[1], X_scaled_lstm.shape[2]))
final_lstm.fit(X_scaled_lstm, y,
               epochs=50, batch_size=32, verbose=0,
               validation_data=(X_scaled_lstm, y), # Using full data for validation here for simplicity
               callbacks=[early_stopping, lr_scheduler])

# Get predictions from final base models
final_rf_preds = final_rf.predict(X_scaled)
final_xgb_preds = final_xgb.predict(X_scaled)
final_lstm_preds = final_lstm.predict(X_scaled_lstm).flatten()

# Stack final base predictions and get final ensemble prediction
final_stacked_X = np.column_stack((final_rf_preds, final_xgb_preds, final_lstm_preds))
final_ensemble_preds = meta_model.predict(final_stacked_X)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error
import seaborn as sns

def evaluate_predictions(y_true, y_pred, model_name):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    # Check for zero true values before calculating MAPE to avoid division by zero
    # Replace zeros with a small epsilon or handle differently if many zeros are expected.
    # For now, filter out zero actuals to avoid error.
    y_true_for_mape = y_true[y_true != 0]
    y_pred_for_mape = y_pred[y_true != 0]
    mape = np.nan # Initialize as NaN
    if len(y_true_for_mape) > 0:
        mape = np.mean(np.abs((y_true_for_mape - y_pred_for_mape) / y_true_for_mape)) * 100 # MAPE in percentage
    else:
        print(f"Warning: Cannot calculate MAPE for {model_name} as all actual values are zero.")


    print(f"\n{model_name} Performance Metrics:")
    print(f"  RMSE: {rmse:.4f}")
    print(f"  MAE: {mae:.4f}")
    print(f"  R²: {r2:.4f}")
    if not np.isnan(mape):
        print(f"  MAPE: {mape:.2f}%")
    else:
        print(f"  MAPE: Not applicable (due to zero actuals)")

    # Visualization
    plt.figure(figsize=(18, 6))

    # Actual vs Predicted Plot
    plt.subplot(1, 2, 1)
    plt.scatter(y_true, y_pred, alpha=0.5, color='blue', label='Predictions')
    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2, label='Perfect Prediction')
    plt.xlabel('Actual Rainfall (mm)', fontsize=12)
    plt.ylabel('Predicted Rainfall (mm)', fontsize=12)
    plt.title(f'{model_name}: Actual vs Predicted Rainfall', fontsize=14)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend()
    plt.gca().set_aspect('equal', adjustable='box')

    # Residuals Distribution Plot
    plt.subplot(1, 2, 2)
    residuals = y_pred - y_true
    sns.histplot(residuals, kde=True, color='purple', bins=30, edgecolor='black')
    plt.xlabel('Residuals (Predicted - Actual)', fontsize=12)
    plt.ylabel('Frequency', fontsize=12)
    plt.title(f'{model_name}: Residuals Distribution', fontsize=14)
    plt.grid(True, linestyle='--', alpha=0.6)

    plt.tight_layout()
    plt.show()

# Evaluate the final ensemble model on the entire dataset
print("--- Ensemble Model Final Performance Metrics (on entire dataset) ---")
evaluate_predictions(y, final_ensemble_preds, "Final Ensemble Model")

# --- Evaluate Standalone Models for comparison ---
print("\n--- Standalone Models Performance Metrics (on out-of-fold predictions) ---")

# Evaluate individual RF model performance from CV folds
rf_oof_preds = np.concatenate(rf_preds)
evaluate_predictions(y_true_stacked, rf_oof_preds, "Random Forest Standalone")

# Evaluate individual XGBoost model performance from CV folds
xgb_oof_preds = np.concatenate(xgb_preds)
evaluate_predictions(y_true_stacked, xgb_oof_preds, "XGBoost Standalone")

# Evaluate individual LSTM model performance from CV folds
lstm_oof_preds = np.concatenate(lstm_preds)
evaluate_predictions(y_true_stacked, lstm_oof_preds, "LSTM Standalone")

# Line chart to visualize the prediction result with the actual dataset
plt.figure(figsize=(12, 7))
plt.plot(y.index, y, label='Actual Rainfall', color='blue', alpha=0.7)
plt.plot(y.index, final_ensemble_preds, label='Ensemble Predictions', color='red', linestyle='--', alpha=0.7)
plt.title('Actual vs Predicted Rainfall Over Time (Ensemble Model)', fontsize=16)
plt.xlabel('Date', fontsize=12)
plt.ylabel('Rainfall (mm)', fontsize=12)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

plt.figure(figsize=(12, 7))
plt.plot(y.index, y, label='Actual Rainfall', color='blue', alpha=0.7)
plt.plot(y.index, final_rf_preds, label='RF Predictions', color='green', linestyle='--', alpha=0.7)
plt.title('Actual vs Predicted Rainfall Over Time (Random Forest)', fontsize=16)
plt.xlabel('Date', fontsize=12)
plt.ylabel('Rainfall (mm)', fontsize=12)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

plt.figure(figsize=(12, 7))
plt.plot(y.index, y, label='Actual Rainfall', color='blue', alpha=0.7)
plt.plot(y.index, final_xgb_preds, label='XGB Predictions', color='purple', linestyle='--', alpha=0.7)
plt.title('Actual vs Predicted Rainfall Over Time (XGBoost)', fontsize=16)
plt.xlabel('Date', fontsize=12)
plt.ylabel('Rainfall (mm)', fontsize=12)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

plt.figure(figsize=(12, 7))
plt.plot(y.index, y, label='Actual Rainfall', color='blue', alpha=0.7)
plt.plot(y.index, final_lstm_preds, label='LSTM Predictions', color='orange', linestyle='--', alpha=0.7)
plt.title('Actual vs Predicted Rainfall Over Time (LSTM)', fontsize=16)
plt.xlabel('Date', fontsize=12)
plt.ylabel('Rainfall (mm)', fontsize=12)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

"""# **11. Advanced Evaluation Metrics and Visualization**"""

import shap

print("--- SHAP Explainability for XGBoost ---")
try:
    explainer = shap.TreeExplainer(final_xgb)
    # Using a subset for SHAP to avoid very long computation if X is very large
    sample_X = X.sample(n=min(200, len(X)), random_state=42)
    shap_values = explainer.shap_values(sample_X)

    # Create a shap.Explanation object for plotting
    shap_explanation = shap.Explanation(
        values=shap_values,
        base_values=explainer.expected_value,
        data=sample_X.values,
        feature_names=X.columns.tolist() # Use the column names from the DataFrame X
    )

    shap.plots.beeswarm(shap_explanation, max_display=10, show=False)
    plt.title("SHAP Feature Importance for XGBoost")
    plt.tight_layout()
    plt.show()
except Exception as e:
    print(f"SHAP plotting failed: {e}. Ensure XGBoost model is tree-based and SHAP package is compatible.")

# import joblib

# print("\n--- Saving Models ---")
# try:
#     joblib.dump(final_rf, 'rf_model_final.pkl')
#     joblib.dump(final_xgb, 'xgb_model_final.pkl')
#     final_lstm.save('lstm_model_final.h5') # Keras models are saved differently
#     joblib.dump(meta_model, 'meta_model_final.pkl')
#     joblib.dump(scaler, 'scaler_final.pkl')
#     print("All models and scaler saved successfully.")
# except Exception as e:
#     print(f"Error saving models: {e}")